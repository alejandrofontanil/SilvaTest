import os
import json
from dotenv import load_dotenv
from google.cloud import aiplatform
from langchain_google_vertexai import VertexAIEmbeddings, VertexAI
from langchain_pinecone import Pinecone
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from google.oauth2 import service_account 

# Carga las variables de entorno
load_dotenv()

# Variables de Pinecone
PINECONE_API_KEY = os.getenv("PINECONE_API_KEY")
INDEX_NAME = "silvatest-rag" 

# Variables de Vertex AI
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
GCP_REGION = 'us-central1' # Regi√≥n estable para Vertex AI
GOOGLE_CREDS_JSON = os.getenv("GOOGLE_CREDS_JSON") 

# Modelo con enfoque en velocidad y calidad
LLM_MODEL_NAME = "gemini-2.5-flash" 

# --- CONFIGURACI√ìN DE CREDENCIALES ---
try:
    credentials = None
    if GOOGLE_CREDS_JSON:
        creds_info = json.loads(GOOGLE_CREDS_JSON)
        credentials = service_account.Credentials.from_service_account_info(creds_info)
        
    aiplatform.init(project=GCP_PROJECT_ID, location=GCP_REGION, credentials=credentials)
    print("‚úÖ Vertex AI inicializado para el agente RAG.")

except Exception as e:
    print(f"Error al inicializar Vertex AI en rag_agent.py: {e}")

# Diccionario de templates de prompts con Meta Prompting Potente
PROMPT_TEMPLATES = {
    "formal": """
        ## üéØ Meta Prompt: Modo Formal (Precisi√≥n Legal)

        Eres un asistente RAG especializado en el temario oficial (Oposici√≥n Agente Medioambiental de Castilla y Le√≥n/Asturias).
        Tu rol es actuar como un **experto legal y t√©cnico**.

        **REGLAS ESTRICTAS:**
        1.  **Exclusividad del Contexto:** Responde √öNICA y EXCLUSIVAMENTE con la informaci√≥n contenida en la secci√≥n {context}.
        2.  **Concisi√≥n:** S√© lo m√°s breve y directo posible. Usa una lista si la informaci√≥n lo permite, pero sin pre√°mbulos.
        3.  **Tono:** Objetivo, t√©cnico y neutro.

        Contexto:
        {context}

        Pregunta del usuario: {question}

        Respuesta concisa:
    """,
    "didactico": """
        ## üßë‚Äçüè´ Meta Prompt: Modo Did√°ctico (Tutor√≠a de Oposiciones)

        Eres Silva, un preparador de oposiciones de √©lite con una personalidad **motivadora y did√°ctica**. Tu misi√≥n es transformar el contexto en **apuntes de estudio memorables**.

        **REGLAS Y FORMATO (Notebook Style):**
        1.  **Formato:** Utiliza Markdown enriquecido (##, vi√±etas, **negritas**) para crear secciones.
        2.  **Estructura:** La respuesta DEBE contener estas secciones (o las m√°s relevantes):
            * **## üìñ Concepto Clave:** Define el t√©rmino principal.
            * **## üêü Ejemplo/Aplicaci√≥n:** Proporciona un ejemplo pr√°ctico del temario.
            * **## üí° Nota de Estudio:** A√±ade un dato relacionado o una diferencia clave para memorizar.
        3.  **Fuentes Legales:** Finaliza siempre con una secci√≥n `## üìö Fuentes Legales/Temario` donde **identificas y nombras** el documento o la ley de origen (ej: "Ley 42/2007 de Patrimonio Natural", "Tema 8. Especies de Pesca (nuevo)"). No uses nombres de archivo crudos como "documento-1.pdf".

        Contexto para el an√°lisis: {context}

        Pregunta del usuario: {question}

        Respuesta en formato de estudio:
    """
}

# La funci√≥n principal
def get_rag_response(query: str, mode: str = "formal"):
    """
    Busca en el √≠ndice de Pinecone y genera una respuesta con Vertex AI.
    Permite seleccionar el modo de respuesta (formal o did√°ctico).
    """
    try:
        credentials = service_account.Credentials.from_service_account_info(json.loads(GOOGLE_CREDS_JSON))
    except Exception:
        credentials = None 
        
    try:
        # --- AJUSTE DE PAR√ÅMETROS CLAVE ---
        # Temperatura: Baja (0.3) para m√°xima precisi√≥n y m√≠nima creatividad, ideal para oposiciones.
        TEMPERATURE = 0.3
        
        # Inicializa los embeddings y el LLM, pasando las credenciales
        embeddings_model = VertexAIEmbeddings(model_name="text-embedding-004", credentials=credentials)
        llm = VertexAI(
            model_name=LLM_MODEL_NAME, 
            credentials=credentials,
            temperature=TEMPERATURE # A√ëADIDO: Control de temperatura
        )

        # Conexi√≥n a Pinecone (ya corregida)
        vectorstore = Pinecone.from_existing_index(
            index_name=INDEX_NAME, 
            embedding=embeddings_model
        )
        
        # Retrieval: k=5 para obtener 5 fragmentos, asegurando suficiente contexto.
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) 
        
        prompt_template_str = PROMPT_TEMPLATES.get(mode, PROMPT_TEMPLATES["formal"])
        prompt = PromptTemplate(template=prompt_template_str, input_variables=["context", "question"])

        # Creaci√≥n de la cadena RAG
        qa = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            chain_type_kwargs={"prompt": prompt},
            return_source_documents=True 
        )
        
        response = qa.invoke({"query": query})
        
        # La l√≥gica de fuentes se mantiene aqu√≠, pero el PROMPT le indica a Gemini
        # c√≥mo formatear el nombre de la fuente de manera amigable.
        sources = sorted(list(set([doc.metadata.get('tema', doc.metadata.get('source', 'Fuente no identificada')) for doc in response.get('source_documents', [])])))
        
        # Instruir al LLM para que filtre y presente las fuentes de manera amigable
        # Enviamos las fuentes de vuelta al LLM para que las cite limpiamente dentro del formato.
        final_result = response.get('result', "No se encontr√≥ una respuesta.")
        
        # Si el modo es did√°ctico, adjuntamos la informaci√≥n de la fuente de forma legible
        if mode == "didactico" and sources:
             final_result += "\n\n" + "## üìö Fuentes Legales/Temario:\n" + "\n".join([f"- {s}" for s in sources])

        return {
            "result": final_result,
            "sources": sources
        }

    except Exception as e:
        print(f"Error en get_rag_response: {e}")
        return {
            "result": f"Error: No se pudo conectar a Google AI. Detalles: {e}",
            "sources": []
        }
