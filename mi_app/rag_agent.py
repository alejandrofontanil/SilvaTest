import os
import json
from dotenv import load_dotenv
from google.oauth2 import service_account
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import re

# LangChain: Importaciones actualizadas
from langchain_google_vertexai import VertexAI
from langchain_google_community import VertexAISearchRetriever

# Carga las variables de entorno
load_dotenv()

# --- CONFIGURACIÃ“N ---
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
DATA_STORE_ID = os.getenv("GCP_DATA_STORE_ID")
LLM_MODEL_NAME = "gemini-1.0-pro"

PROMPT_TEMPLATES = {
    "formal": """
        ## ðŸŽ¯ Meta Prompt: Modo Formal (PrecisiÃ³n Legal)
        Eres un asistente RAG especializado en el temario oficial de Agente Medioambiental. Tu rol es actuar como un experto legal y tÃ©cnico.
        REGLAS ESTRICTAS:
        1.  **Exclusividad del Contexto:** Responde ÃšNICA y EXCLUSIVAMENTE con la informaciÃ³n contenida en la secciÃ³n {context}.
        2.  **ConcisiÃ³n:** SÃ© lo mÃ¡s breve y directo posible.
        3.  **Tono:** Objetivo, tÃ©cnico y neutro.

        Contexto:
        {context}
        Pregunta del usuario: {question}
        Respuesta concisa:
    """,
    "didactico": """
        ## ðŸ§‘â€ðŸ« Meta Prompt: Modo DidÃ¡ctico (TutorÃ­a de Oposiciones)
        Eres Silva, un preparador de oposiciones motivador y didÃ¡ctico. Tu misiÃ³n es transformar el contexto en apuntes de estudio memorables.
        REGLAS Y FORMATO (Notebook Style):
        1.  **Formato:** Utiliza Markdown enriquecido (##, viÃ±etas, **negritas**).
        2.  **Estructura:** La respuesta DEBE contener estas secciones (o las mÃ¡s relevantes):
            * **## ðŸ“– Concepto Clave:** Define el tÃ©rmino principal.
            * **## ðŸŸ Ejemplo/AplicaciÃ³n:** Proporciona un ejemplo prÃ¡ctico del temario.
            * **## ðŸ’¡ Nota de Estudio:** AÃ±ade un dato relacionado o una diferencia clave para memorizar.
        3.  **No incluyas "Fuentes":** No incluyas una secciÃ³n final sobre "Fuentes" o "Temario". El sistema lo aÃ±adirÃ¡ automÃ¡ticamente.
        4.  **Inicio Directo:** Comienza la respuesta directamente con el primer tÃ­tulo `## ðŸ“– Concepto Clave`.

        Contexto para el anÃ¡lisis: {context}
        Pregunta del usuario: {question}
        Respuesta en formato de estudio:
    """
}

# --- INICIALIZACIÃ“N DE CREDENCIALES (NUEVA VERSIÃ“N) ---
SECRET_FILE_PATH = "/etc/secrets/gcp_service_account_key.json"
credentials = None
try:
    if os.path.exists(SECRET_FILE_PATH):
        credentials = service_account.Credentials.from_service_account_file(SECRET_FILE_PATH)
        print("âœ… Credenciales de Google Cloud cargadas desde Secret File.")
    else:
        # LÃ³gica para desarrollo local, leyendo la variable de entorno
        GOOGLE_CREDS_JSON = os.getenv("GOOGLE_CREDS_JSON")
        if GOOGLE_CREDS_JSON:
            creds_info = json.loads(GOOGLE_CREDS_JSON)
            credentials = service_account.Credentials.from_service_account_info(creds_info)
            print("âœ… Credenciales de Google Cloud cargadas desde variable de entorno.")
        else:
            print("âŒ No se encontrÃ³ Secret File ni variable de entorno para las credenciales.")
except Exception as e:
    print(f"âŒ ERROR al cargar las credenciales de Google Cloud: {e}")

# --- FUNCIÃ“N DE LIMPIEZA DE FUENTES ---
def clean_source_name(source_path: str) -> str:
    if not isinstance(source_path, str): return 'Fuente no identificada'
    file_name = source_path.split('/')[-1]
    cleaned_name = re.sub(r'\.(pdf|txt)$', '', file_name, flags=re.IGNORECASE)
    cleaned_name = cleaned_name.replace('_', ' ').replace('.', ' ').strip()
    return cleaned_name.title()

# --- FUNCIÃ“N PRINCIPAL DEL AGENTE RAG (VERSIÃ“N DE DEPURACIÃ“N) ---
def get_rag_response(query: str, mode: str = "formal", selected_sources: list | None = None):
    if not credentials:
        return {"result": "Error crÃ­tico: Las credenciales de Google Cloud no estÃ¡n configuradas en el servidor.", "sources": []}

    try:
        TEMPERATURE = 0.2
        RETRIEVAL_K = 5

        llm = VertexAI(
            model_name=LLM_MODEL_NAME,
            credentials=credentials,
            temperature=TEMPERATURE,
            project=GCP_PROJECT_ID,
            location="europe-west1"
        )

        # --- CAMBIO TEMPORAL: Desactivamos el filtro para que la consulta funcione ---
        filter_string = None
        # if selected_sources and isinstance(selected_sources, list):
        #     quoted_sources = [f'"{source}"' for source in selected_sources]
        #     filter_string = " OR ".join(f"gcs_uri:{s}" for s in quoted_sources)

        retriever = VertexAISearchRetriever(
            project_id=GCP_PROJECT_ID,
            data_store_id=DATA_STORE_ID,
            credentials=credentials,
            filter=filter_string,
            max_documents=RETRIEVAL_K,
        )
        
        prompt_template_str = PROMPT_TEMPLATES.get(mode, PROMPT_TEMPLATES["formal"])
        prompt = PromptTemplate(template=prompt_template_str, input_variables=["context", "question"])

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm, chain_type="stuff", retriever=retriever,
            chain_type_kwargs={"prompt": prompt}, return_source_documents=True
        )
        
        response = qa_chain.invoke({"query": query})
        
        raw_sources = response.get('source_documents', [])

        # --- DIAGNÃ“STICO: Imprimimos los metadatos en los logs ---
        if raw_sources:
            print("--- METADATOS DEL PRIMER DOCUMENTO ENCONTRADO ---")
            print(raw_sources[0].metadata)
            print("-------------------------------------------------")
        
        cleaned_sources = []
        if raw_sources:
            for doc in raw_sources:
                source_uri = doc.metadata.get('source')
                if source_uri:
                    cleaned_sources.append(clean_source_name(source_uri))
        cleaned_sources = sorted(list(set(cleaned_sources)))

        final_result = response.get('result', "No se encontrÃ³ una respuesta.").strip()
        
        if cleaned_sources:
            if mode == "didactico":
                final_result += "\n\n" + "## ðŸ“š Fuentes Legales/Temario\n" + "\n".join([f"- {s}" for s in cleaned_sources])
            else:
                final_result += "\n\n[Fuentes consultadas: " + "; ".join(cleaned_sources) + "]"

        return {"result": final_result, "sources": cleaned_sources}

    except Exception as e:
        print(f"Error en get_rag_response: {e}")
        return {"result": f"Error: No se pudo conectar a Google AI. Detalles: {e}", "sources": []}