# rag_agent.py (VERSI√ìN FINAL)
import os
import json
from dotenv import load_dotenv
from google.oauth2 import service_account
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import re

# LangChain: Importaciones actualizadas para corregir DeprecationWarning
from langchain_google_vertexai import VertexAI
from langchain_google_community import VertexAISearchRetriever # <-- Importaci√≥n corregida

# Carga las variables de entorno
load_dotenv()

# --- CONFIGURACI√ìN ---
GCP_PROJECT_ID = os.getenv("GCP_PROJECT_ID")
LOCATION = "global"
DATA_STORE_ID = os.getenv("GCP_DATA_STORE_ID")
GOOGLE_CREDS_JSON = os.getenv("GOOGLE_CREDS_JSON")
LLM_MODEL_NAME = "gemini-1.5-flash-001"
PROMPT_TEMPLATES = {
    "formal": """...""", # Mant√©n tus prompts como estaban
    "didactico": """..."""
}
PROMPT_TEMPLATES = {
    "formal": """
        ## üéØ Meta Prompt: Modo Formal (Precisi√≥n Legal)
        Eres un asistente RAG especializado en el temario oficial de Agente Medioambiental. Tu rol es actuar como un experto legal y t√©cnico.
        REGLAS ESTRICTAS:
        1.  **Exclusividad del Contexto:** Responde √öNICA y EXCLUSIVAMENTE con la informaci√≥n contenida en la secci√≥n {context}.
        2.  **Concisi√≥n:** S√© lo m√°s breve y directo posible.
        3.  **Tono:** Objetivo, t√©cnico y neutro.

        Contexto:
        {context}
        Pregunta del usuario: {question}
        Respuesta concisa:
    """,
    "didactico": """
        ## üßë‚Äçüè´ Meta Prompt: Modo Did√°ctico (Tutor√≠a de Oposiciones)
        Eres Silva, un preparador de oposiciones motivador y did√°ctico. Tu misi√≥n es transformar el contexto en apuntes de estudio memorables.
        REGLAS Y FORMATO (Notebook Style):
        1.  **Formato:** Utiliza Markdown enriquecido (##, vi√±etas, **negritas**).
        2.  **Estructura:** La respuesta DEBE contener estas secciones (o las m√°s relevantes):
            * **## üìñ Concepto Clave:** Define el t√©rmino principal.
            * **## üêü Ejemplo/Aplicaci√≥n:** Proporciona un ejemplo pr√°ctico del temario.
            * **## üí° Nota de Estudio:** A√±ade un dato relacionado o una diferencia clave para memorizar.
        3.  **No incluyas "Fuentes":** No incluyas una secci√≥n final sobre "Fuentes" o "Temario". El sistema lo a√±adir√° autom√°ticamente.
        4.  **Inicio Directo:** Comienza la respuesta directamente con el primer t√≠tulo `## üìñ Concepto Clave`.

        Contexto para el an√°lisis: {context}
        Pregunta del usuario: {question}
        Respuesta en formato de estudio:
    """
}


# --- INICIALIZACI√ìN DE CREDENCIALES (FUERA DE LA FUNCI√ìN) ---
credentials = None
if GOOGLE_CREDS_JSON:
    try:
        creds_info = json.loads(GOOGLE_CREDS_JSON)
        credentials = service_account.Credentials.from_service_account_info(creds_info)
        print("‚úÖ Credenciales de Google Cloud cargadas correctamente.")
    except Exception as e:
        print(f"‚ùå ERROR al cargar las credenciales de Google Cloud: {e}")

def clean_source_name(source_path: str) -> str:
    # ... tu funci√≥n de limpieza sin cambios ...
    if not isinstance(source_path, str): return 'Fuente no identificada'
    file_name = source_path.split('/')[-1]
    cleaned_name = re.sub(r'\.(pdf|txt)$', '', file_name, flags=re.IGNORECASE)
    cleaned_name = cleaned_name.replace('_', ' ').replace('.', ' ').strip()
    return cleaned_name.title()


# --- FUNCI√ìN PRINCIPAL DEL AGENTE RAG ---
def get_rag_response(query: str, mode: str = "formal", selected_sources: list | None = None):
    # Verificamos que las credenciales se cargaron al inicio
    if not credentials:
        return {"result": "Error cr√≠tico: Las credenciales de Google Cloud no est√°n configuradas en el servidor.", "sources": []}

    try:
        TEMPERATURE = 0.2
        RETRIEVAL_K = 5

        # Pasamos las credenciales expl√≠citamente a cada componente
        llm = VertexAI(
            model_name=LLM_MODEL_NAME,
            credentials=credentials,
            temperature=TEMPERATURE,
            project=GCP_PROJECT_ID # Buena pr√°ctica a√±adir el project_id
        )

        filter_string = None
        if selected_sources and isinstance(selected_sources, list):
            quoted_sources = [f'"{source}"' for source in selected_sources]
            filter_string = " OR ".join(f"source:{s}" for s in quoted_sources)

        # Usamos la clase corregida y le pasamos las credenciales
        retriever = VertexAISearchRetriever(
            project_id=GCP_PROJECT_ID,
            location=LOCATION,
            data_store_id=DATA_STORE_ID,
            credentials=credentials, # <-- Muy importante pasar las credenciales aqu√≠
            filter=filter_string,
            max_documents=RETRIEVAL_K,
        )
        
        prompt_template_str = PROMPT_TEMPLATES.get(mode, PROMPT_TEMPLATES["formal"])
        prompt = PromptTemplate(template=prompt_template_str, input_variables=["context", "question"])

        qa_chain = RetrievalQA.from_chain_type(
            llm=llm, chain_type="stuff", retriever=retriever,
            chain_type_kwargs={"prompt": prompt}, return_source_documents=True
        )
        
        response = qa_chain.invoke({"query": query})
        
        raw_sources = response.get('source_documents', [])
        cleaned_sources = sorted(list(set([clean_source_name(doc.metadata.get('source', '')) for doc in raw_sources if doc.metadata.get('source')])))
        final_result = response.get('result', "No se encontr√≥ una respuesta.").strip()
        
        if cleaned_sources:
            if mode == "didactico":
                final_result += "\n\n" + "## üìö Fuentes Legales/Temario\n" + "\n".join([f"- {s}" for s in cleaned_sources])
            else:
                final_result += "\n\n[Fuentes consultadas: " + "; ".join(cleaned_sources) + "]"

        return {"result": final_result, "sources": cleaned_sources}

    except Exception as e:
        print(f"Error en get_rag_response: {e}")
        return {"result": f"Error: No se pudo conectar a Google AI. Detalles: {e}", "sources": []}